{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Preambule**"
      ],
      "metadata": {
        "id": "IP1hNDmcd-wL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LWVMiIcNQufo"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "69JLKXxgRLDc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "reot3uauRTpS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "import sys"
      ],
      "metadata": {
        "id": "t31c33AGR0nV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xXGhtKvR4o8",
        "outputId": "5668e679-9b20-4a15-f8a3-990ef33bbc51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.2.3\n",
            "  Downloading pyspark-3.2.3.tar.gz (281.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.3-py2.py3-none-any.whl size=281990673 sha256=3620b14e68feb65629d88541dee8d32be9dc5f0ebdcc6cfd08c4be81166cf60a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/99/8c/e2d5ede0e1aefb33c64af344f2cd569354237f0bdd673bd243\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import findspark\n",
        "#findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "4NKF2zToTGUm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuR6LFEsTUCK",
        "outputId": "6b373d2c-ddca-4031-cbd8-e8ba62a34123"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"MyFirstProgram\").getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "\n",
        "# Test the spark\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKlM0yRiTbj0",
        "outputId": "d1fdf597-60d5-4f9b-d849-42c8bb4e5e7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Exercise n°0 : Prepare your documents**"
      ],
      "metadata": {
        "id": "EB69ZLDieRKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Download all the text files\n",
        "import time\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "RJ_url = \"https://www.gutenberg.org/files/1112/1112.txt\"\n",
        "spark.sparkContext.addFile(RJ_url)\n",
        "RomeoJuliet_rdd=sc.textFile(SparkFiles.get(\"1112.txt\"))\n",
        "\n",
        "Hamlet_url = \"https://www.gutenberg.org/files/1524/1524-0.txt\"\n",
        "spark.sparkContext.addFile(Hamlet_url)\n",
        "Hamlet_rdd=sc.textFile(SparkFiles.get(\"1524-0.txt\"))\n",
        "\n",
        "Richard_url = \"https://www.gutenberg.org/cache/epub/1776/pg1776.txt\"\n",
        "spark.sparkContext.addFile(Richard_url)\n",
        "Richard_rdd=sc.textFile(SparkFiles.get(\"pg1776.txt\"))"
      ],
      "metadata": {
        "id": "5TCOaAZVeXgz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Create the cleaning function\n",
        "\n",
        "import string\n",
        "\n",
        "def cleaningFunc(text):\n",
        "  text = text.lower()\n",
        "  text = text.translate(text.maketrans(\"\",\"\",string.punctuation))\n",
        "  return text"
      ],
      "metadata": {
        "id": "Tac01jBS_2o9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Clean all the texts\n",
        "start = time.time()\n",
        "\n",
        "RJ_clean = RomeoJuliet_rdd.flatMap(lambda line : line.split(\" \")).map(cleaningFunc)\n",
        "\n",
        "Hamlet_clean = Hamlet_rdd.flatMap(lambda line : line.split(\" \")).map(cleaningFunc)\n",
        "\n",
        "Richard_clean = Richard_rdd.flatMap(lambda line : line.split(\" \")).map(cleaningFunc)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "Richard_clean.take(20)"
      ],
      "metadata": {
        "id": "9WPGUgRDSU8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4192a939-bcd4-410b-c98e-65643bfb6e08"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.011086225509643555\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'this',\n",
              " 'etext',\n",
              " 'file',\n",
              " 'is',\n",
              " 'presented',\n",
              " 'by',\n",
              " 'project',\n",
              " 'gutenberg',\n",
              " 'in',\n",
              " 'cooperation',\n",
              " 'with',\n",
              " 'world',\n",
              " 'library',\n",
              " 'inc',\n",
              " 'from',\n",
              " 'their',\n",
              " 'library',\n",
              " 'of',\n",
              " 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# **Exercise n°1 : Count Words**\n"
      ],
      "metadata": {
        "id": "2Abu7UtnS4bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Count the (filtered) words of each documents\n",
        "\n",
        "import re\n",
        "import string\n",
        "from pyspark import SparkContext\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "RJ_CountWords = RJ_clean.filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, 1)).reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "Hamlet_CountWords = Hamlet_clean.filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, 1)).reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "Richard_CountWords = Richard_clean.filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, 1)).reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "Richard_CountWords.take(10)"
      ],
      "metadata": {
        "id": "5YEf-exHS6uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42600d56-539d-4321-c2d0-80b3de1aa4ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.20787382125854492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 204),\n",
              " ('is', 296),\n",
              " ('presented', 1),\n",
              " ('project', 26),\n",
              " ('gutenberg', 21),\n",
              " ('in', 306),\n",
              " ('cooperation', 1),\n",
              " ('world', 31),\n",
              " ('library', 16),\n",
              " ('inc', 11)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Exercice n°2 : Finding Frequent Terms and Stop Words**"
      ],
      "metadata": {
        "id": "1F4FHweNdICz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### Import the list of stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CDEbvY8dPbM",
        "outputId": "d8d8f0bb-fbba-4ea4-98b5-7e417e43da5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Keep only the stopwords from the text files\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "RJ_StopWords = RJ_clean.filter(lambda x : x in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, 1)).reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "Hamlet_StopWords = Hamlet_clean.filter(lambda x : x in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, 1)).reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "Richard_StopWords = Richard_clean.filter(lambda x : x in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, 1)).reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "RJ_StopWords.take(10)"
      ],
      "metadata": {
        "id": "U6ec6yp-dxG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb4d999-1b4e-4e86-b413-fe63277650a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0829305648803711\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('of', 535),\n",
              " ('this', 279),\n",
              " ('was', 51),\n",
              " ('at', 87),\n",
              " ('when', 55),\n",
              " ('there', 67),\n",
              " ('is', 375),\n",
              " ('an', 91),\n",
              " ('as', 167),\n",
              " ('no', 114)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Create a rdd with all the stopwords of the 3 texts\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "all_StopWords_V1 = RJ_StopWords.union(Hamlet_StopWords).union(Richard_StopWords).reduceByKey(lambda x, y : x + y).sortBy(lambda row : row[1], ascending=False)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "all_StopWords_V1.take(30)"
      ],
      "metadata": {
        "id": "Im0cL2hrhhmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d39e66-3f92-4ff5-b1b1-2c108e08e670"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2852823734283447\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 3004),\n",
              " ('and', 2636),\n",
              " ('to', 2049),\n",
              " ('of', 1961),\n",
              " ('i', 1588),\n",
              " ('a', 1440),\n",
              " ('my', 1342),\n",
              " ('in', 1191),\n",
              " ('you', 1180),\n",
              " ('that', 1067),\n",
              " ('is', 1032),\n",
              " ('with', 877),\n",
              " ('it', 848),\n",
              " ('not', 841),\n",
              " ('this', 824),\n",
              " ('for', 764),\n",
              " ('his', 675),\n",
              " ('me', 672),\n",
              " ('be', 662),\n",
              " ('but', 616),\n",
              " ('as', 552),\n",
              " ('your', 505),\n",
              " ('he', 482),\n",
              " ('what', 481),\n",
              " ('so', 479),\n",
              " ('have', 456),\n",
              " ('or', 456),\n",
              " ('will', 434),\n",
              " ('by', 412),\n",
              " ('him', 410)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "all_StopWords = all_StopWords_V1.map(lambda x : (x[1],x[0]))\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "all_StopWords.take(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MujhK22r0qmr",
        "outputId": "9ae0d4fa-99a9-401f-be53-f9502f5dd817"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00017118453979492188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3004, 'the'),\n",
              " (2636, 'and'),\n",
              " (2049, 'to'),\n",
              " (1961, 'of'),\n",
              " (1588, 'i'),\n",
              " (1440, 'a'),\n",
              " (1342, 'my'),\n",
              " (1191, 'in'),\n",
              " (1180, 'you'),\n",
              " (1067, 'that'),\n",
              " (1032, 'is'),\n",
              " (877, 'with'),\n",
              " (848, 'it'),\n",
              " (841, 'not'),\n",
              " (824, 'this'),\n",
              " (764, 'for'),\n",
              " (675, 'his'),\n",
              " (672, 'me'),\n",
              " (662, 'be'),\n",
              " (616, 'but'),\n",
              " (552, 'as'),\n",
              " (505, 'your'),\n",
              " (482, 'he'),\n",
              " (481, 'what'),\n",
              " (479, 'so'),\n",
              " (456, 'have'),\n",
              " (456, 'or'),\n",
              " (434, 'will'),\n",
              " (412, 'by'),\n",
              " (410, 'him')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Create the dataframe\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "columns = [\"count\",\"stopword\"]\n",
        "\n",
        "allStopWordsDF = spark.createDataFrame(all_StopWords, columns)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "allStopWordsDF.show()"
      ],
      "metadata": {
        "id": "uuvGjKB8mgCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ac3f88-4765-46d8-a795-5d92e586523d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3372330665588379\n",
            "+-----+--------+\n",
            "|count|stopword|\n",
            "+-----+--------+\n",
            "| 3004|     the|\n",
            "| 2636|     and|\n",
            "| 2049|      to|\n",
            "| 1961|      of|\n",
            "| 1588|       i|\n",
            "| 1440|       a|\n",
            "| 1342|      my|\n",
            "| 1191|      in|\n",
            "| 1180|     you|\n",
            "| 1067|    that|\n",
            "| 1032|      is|\n",
            "|  877|    with|\n",
            "|  848|      it|\n",
            "|  841|     not|\n",
            "|  824|    this|\n",
            "|  764|     for|\n",
            "|  675|     his|\n",
            "|  672|      me|\n",
            "|  662|      be|\n",
            "|  616|     but|\n",
            "+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Create the csv file\n",
        "start = time.time()\n",
        "\n",
        "allStopWordsFormatCSV = allStopWordsDF.write.csv(\"StopWord\")\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n"
      ],
      "metadata": {
        "id": "iSy-S8Urnxyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466ed0f1-80d9-4738-b9eb-4c3af18888d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.290879964828491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **Exercise n°3 : Simple Inverted Index**"
      ],
      "metadata": {
        "id": "VnAp4BsYrKQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Delete the stopwords from the text files\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "RJ_OtherWords = RJ_clean.filter(lambda x : x not in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, \"RJ.txt\")).reduceByKey(lambda x, y : x )\n",
        "\n",
        "Hamlet_OtherWords = Hamlet_clean.filter(lambda x : x not in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, \"Hamlet.txt\")).reduceByKey(lambda x, y : x)\n",
        "\n",
        "Richard_OtherWords = Richard_clean.filter(lambda x : x not in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, \"Richard.txt\")).reduceByKey(lambda x, y : x)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "Richard_OtherWords.take(10)"
      ],
      "metadata": {
        "id": "9vXOy7NirYAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc89a61-3cdc-4a74-9002-4b0cab429ee4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11527538299560547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('presented', 'Richard.txt'),\n",
              " ('project', 'Richard.txt'),\n",
              " ('gutenberg', 'Richard.txt'),\n",
              " ('cooperation', 'Richard.txt'),\n",
              " ('world', 'Richard.txt'),\n",
              " ('library', 'Richard.txt'),\n",
              " ('inc', 'Richard.txt'),\n",
              " ('shakespeare', 'Richard.txt'),\n",
              " ('cdroms', 'Richard.txt'),\n",
              " ('placed', 'Richard.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Create a rdd with all the other words of the 3 texts\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "all_OtherWords = RJ_OtherWords.union(Hamlet_OtherWords).union(Richard_OtherWords).reduceByKey(lambda x, y : x + \", \" + y).sortByKey().zipWithIndex().map(lambda x : (x[1],x[0]))\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "all_OtherWords.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKgWOIOhs-w1",
        "outputId": "5e1abb29-fc75-4ab1-bd6a-98bf4b1458a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.383584976196289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, ('abate', 'RJ.txt, Hamlet.txt')),\n",
              " (1, ('abatements', 'Hamlet.txt')),\n",
              " (2, ('abbey', 'RJ.txt')),\n",
              " (3, ('abbot', 'Richard.txt')),\n",
              " (4, ('abed', 'RJ.txt')),\n",
              " (5, ('abels', 'Richard.txt')),\n",
              " (6, ('abet', 'Richard.txt')),\n",
              " (7, ('abhorred', 'RJ.txt, Hamlet.txt')),\n",
              " (8, ('abhors', 'RJ.txt')),\n",
              " (9, ('abide', 'RJ.txt, Hamlet.txt, Richard.txt'))]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Exercise n°4 : Extended Inverted Index**"
      ],
      "metadata": {
        "id": "QRGiVpSSwoON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Delete the stopwords from the text files\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "RJ_OtherWords_Extended = RJ_clean.filter(lambda x : x not in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, \"RJ.txt #1\")).reduceByKey(lambda x, y : \"RJ.txt #\"+str(int(x[x.find(\"#\")+1:])+int(y[y.find(\"#\")+1:])) )\n",
        "\n",
        "Hamlet_OtherWords_Extended = Hamlet_clean.filter(lambda x : x not in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, \"Hamlet.txt #1\")).reduceByKey(lambda x, y : \"Hamlet.txt #\"+str(int(x[x.find(\"#\")+1:])+int(y[y.find(\"#\")+1:])) )\n",
        "\n",
        "Richard_OtherWords_Extended = Richard_clean.filter(lambda x : x not in stopwords).filter(lambda x: re.match('[a-z]+', x)).map(lambda word: (word, \"Richard.txt #1\")).reduceByKey(lambda x, y : \"Richard.txt #\"+str(int(x[x.find(\"#\")+1:])+int(y[y.find(\"#\")+1:])) )\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "RJ_OtherWords_Extended.take(10)"
      ],
      "metadata": {
        "id": "lDzQb7zqwtTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45536397-f7c1-430c-dc57-e1e909f86159"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1475062370300293\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('project', 'RJ.txt #90'),\n",
              " ('gutenberg', 'RJ.txt #32'),\n",
              " ('ebook', 'RJ.txt #13'),\n",
              " ('juliet', 'RJ.txt #65'),\n",
              " ('shakespeare', 'RJ.txt #8'),\n",
              " ('files', 'RJ.txt #3'),\n",
              " ('produced', 'RJ.txt #2'),\n",
              " ('proofing', 'RJ.txt #1'),\n",
              " ('tools', 'RJ.txt #1'),\n",
              " ('developed', 'RJ.txt #1')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Create a rdd with all the other words of the 3 texts\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "all_OtherWords_Extended = RJ_OtherWords_Extended.union(Hamlet_OtherWords_Extended).union(Richard_OtherWords_Extended).reduceByKey(lambda x, y : x + \", \" + y).sortByKey().zipWithIndex().map(lambda x : (x[1],x[0]))\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end-start)\n",
        "\n",
        "all_OtherWords_Extended.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c163e1-cd3a-40ec-ab06-cd5d5519a888",
        "id": "e1-AIVUaBhL8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.214059591293335\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, ('abate', 'RJ.txt #1, Hamlet.txt #1')),\n",
              " (1, ('abatements', 'Hamlet.txt #1')),\n",
              " (2, ('abbey', 'RJ.txt #1')),\n",
              " (3, ('abbot', 'Richard.txt #7')),\n",
              " (4, ('abed', 'RJ.txt #1')),\n",
              " (5, ('abels', 'Richard.txt #1')),\n",
              " (6, ('abet', 'Richard.txt #1')),\n",
              " (7, ('abhorred', 'RJ.txt #1, Hamlet.txt #1')),\n",
              " (8, ('abhors', 'RJ.txt #1')),\n",
              " (9, ('abide', 'RJ.txt #1, Hamlet.txt #1, Richard.txt #1'))]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}